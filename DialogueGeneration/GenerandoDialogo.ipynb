{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Diálogos\n",
    "\n",
    "Éste proyecto corresponde al realizado para el Nanodegree Deep Learning de Udacity. En éste notebook entrego una versión más acotada del original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __OBS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjunto una lista con las bibliotecas y sus versiones respectivas ya que pueden haber problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /opt/conda:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_libgcc_mutex             0.1                        main    defaults\r\n",
      "altair                    1.2.1                      py_0    conda-forge\r\n",
      "asn1crypto                0.22.0                   py36_0    conda-forge\r\n",
      "atari-py                  0.1.7                    pypi_0    pypi\r\n",
      "atomicwrites              1.3.0                    pypi_0    pypi\r\n",
      "attrs                     19.1.0                     py_0    conda-forge\r\n",
      "audioread                 2.1.6                    py36_0    conda-forge\r\n",
      "av                        0.3.3                    py36_2    conda-forge\r\n",
      "awscli                    1.16.17                  py36_0    conda-forge\r\n",
      "backcall                  0.1.0                      py_0    conda-forge\r\n",
      "backports                 1.0                      py36_1    conda-forge\r\n",
      "backports.functools_lru_cache 1.4                      py36_1    conda-forge\r\n",
      "backports.weakref         1.0rc1                   py36_1    conda-forge\r\n",
      "beautifulsoup4            4.6.0                    py36_0    conda-forge\r\n",
      "binutils_impl_linux-64    2.31.1               h6176602_1    defaults\r\n",
      "binutils_linux-64         2.31.1               h6176602_7    defaults\r\n",
      "blas                      1.1                    openblas    conda-forge\r\n",
      "bleach                    1.5.0                    py36_0    conda-forge\r\n",
      "blinker                   1.4                        py_0    conda-forge\r\n",
      "bokeh                     0.12.13                  py36_0    conda-forge\r\n",
      "boto                      2.48.0           py36h6e4cd66_1    defaults\r\n",
      "boto3                     1.9.7                      py_0    conda-forge\r\n",
      "botocore                  1.12.7                     py_0    conda-forge\r\n",
      "box2d                     2.3.2                    pypi_0    pypi\r\n",
      "box2d-kengz               2.3.3                    pypi_0    pypi\r\n",
      "bresenham                 0.2                      pypi_0    pypi\r\n",
      "bz2file                   0.98                     py36_0    defaults\r\n",
      "bzip2                     1.0.6                         1    conda-forge\r\n",
      "ca-certificates           2019.11.28           hecc5488_0    conda-forge\r\n",
      "cairo                     1.14.10              h58b644b_4    defaults\r\n",
      "cassandra-driver          3.11.0                   pypi_0    pypi\r\n",
      "certifi                   2019.11.28               py36_0    conda-forge\r\n",
      "cffi                      1.11.2                   py36_0    conda-forge\r\n",
      "chardet                   3.0.4                    py36_0    conda-forge\r\n",
      "clang_variant             1.0               cling_6.14.06    conda-forge\r\n",
      "clangdev                  5.0.0             h935a590_1004    conda-forge\r\n",
      "click                     6.7                        py_1    conda-forge\r\n",
      "cling                     0.5               he860b03_1006    conda-forge\r\n",
      "cloudpickle               0.2.2                    py36_4    conda-forge\r\n",
      "colorama                  0.3.9                      py_1    conda-forge\r\n",
      "colour                    0.1.5                      py_0    conda-forge\r\n",
      "conda                     4.6.14                   py36_0    conda-forge\r\n",
      "cppzmq                    4.3.0             h1b96802_1001    conda-forge\r\n",
      "cryptography              2.1.4                    py36_0    conda-forge\r\n",
      "cryptopp                  7.0.0             hf484d3e_1003    conda-forge\r\n",
      "cudatoolkit               8.0                           3    defaults\r\n",
      "cudnn                     6.0.21                cuda8.0_0    defaults\r\n",
      "curl                      7.49.0                        1    defaults\r\n",
      "cycler                    0.10.0                   py36_0    conda-forge\r\n",
      "cython                    0.29.7                   pypi_0    pypi\r\n",
      "cytoolz                   0.9.0.1         py36h14c3975_1001    conda-forge\r\n",
      "dask-core                 0.16.1                     py_0    conda-forge\r\n",
      "dbus                      1.10.22                       0    conda-forge\r\n",
      "decorator                 4.0.11                   py36_0    conda-forge\r\n",
      "defusedxml                0.5.0                      py_1    conda-forge\r\n",
      "dill                      0.2.7.1                  py36_0    conda-forge\r\n",
      "docutils                  0.14                     py36_0    conda-forge\r\n",
      "dronekit                  2.9.2                    pypi_0    pypi\r\n",
      "entrypoints               0.2.3                    py36_1    conda-forge\r\n",
      "et_xmlfile                1.0.1                    py36_0    conda-forge\r\n",
      "eventlet                  0.22.0                     py_0    conda-forge\r\n",
      "expat                     2.2.5                         0    conda-forge\r\n",
      "fastcache                 1.0.2                    py36_0    conda-forge\r\n",
      "ffmpeg                    3.4.1                         0    conda-forge\r\n",
      "flask                     0.12.2                   py36_0    conda-forge\r\n",
      "flask-socketio            2.9.2                    py36_0    conda-forge\r\n",
      "fontconfig                2.12.4               h88586e7_1    defaults\r\n",
      "freetype                  2.8.1                         0    conda-forge\r\n",
      "future                    0.16.0                   py36_0    conda-forge\r\n",
      "gcc_impl_linux-64         7.3.0                habb00fd_1    conda-forge\r\n",
      "gcc_linux-64              7.3.0                h553295d_7    conda-forge\r\n",
      "gensim                    3.4.0                    py36_0    conda-forge\r\n",
      "geos                      3.6.2                         1    conda-forge\r\n",
      "gettext                   0.19.7                        1    conda-forge\r\n",
      "giflib                    5.1.4                         0    conda-forge\r\n",
      "glfw                      1.8.0                    pypi_0    pypi\r\n",
      "glib                      2.53.6               h0d2cc06_1    defaults\r\n",
      "gmp                       6.1.2                         0    conda-forge\r\n",
      "graphite2                 1.3.13            hf484d3e_1000    conda-forge\r\n",
      "greenlet                  0.4.12                   py36_0    conda-forge\r\n",
      "gst-plugins-base          1.14.0               he9c8b40_0    defaults\r\n",
      "gstreamer                 1.14.0               hb31296c_0    defaults\r\n",
      "gxx_impl_linux-64         7.3.0                hdf63c60_1    conda-forge\r\n",
      "gxx_linux-64              7.3.0                h553295d_7    conda-forge\r\n",
      "gym                       0.9.6                    pypi_0    pypi\r\n",
      "h5py                      2.7.1                    py36_1    conda-forge\r\n",
      "harfbuzz                  1.5.0                h2545bd6_0    defaults\r\n",
      "hdf5                      1.8.18                        2    conda-forge\r\n",
      "heapdict                  1.0.0                    py36_0    conda-forge\r\n",
      "html2text                 2018.1.9                 pypi_0    pypi\r\n",
      "html5lib                  0.9999999                py36_0    conda-forge\r\n",
      "icu                       58.2                          0    conda-forge\r\n",
      "idna                      2.6                      py36_1    conda-forge\r\n",
      "imageio                   2.1.2                    py36_0    conda-forge\r\n",
      "intel-openmp              2018.0.0             hc7b2577_8    defaults\r\n",
      "ipykernel                 4.9.0                    py36_0    conda-forge\r\n",
      "ipython                   6.5.0                    py36_0    conda-forge\r\n",
      "ipython-sql               0.3.9                    py36_0    conda-forge\r\n",
      "ipython_genutils          0.2.0                    py36_0    conda-forge\r\n",
      "ipywidgets                7.0.5                    py36_0    conda-forge\r\n",
      "itsdangerous              0.24                       py_2    conda-forge\r\n",
      "jasper                    1.900.1                       4    conda-forge\r\n",
      "jdc                       0.0.8                    pypi_0    pypi\r\n",
      "jdcal                     1.3                      py36_0    conda-forge\r\n",
      "jedi                      0.10.2                   py36_0    conda-forge\r\n",
      "jieba                     0.39                     pypi_0    pypi\r\n",
      "jinja2                    2.10                     py36_0    conda-forge\r\n",
      "jmespath                  0.9.3                    py36_0    conda-forge\r\n",
      "joblib                    0.11                     py36_0    conda-forge\r\n",
      "jpeg                      9b                            2    conda-forge\r\n",
      "json5                     0.8.5                      py_0    conda-forge\r\n",
      "jsonschema                2.6.0                    py36_0    conda-forge\r\n",
      "jupyter_client            5.2.4                      py_3    conda-forge\r\n",
      "jupyter_core              4.4.0                      py_0    conda-forge\r\n",
      "jupyterlab                1.0.9                    py36_0    conda-forge\r\n",
      "jupyterlab_server         1.0.0                      py_0    defaults\r\n",
      "keras                     2.0.9                    py36_0    conda-forge\r\n",
      "libblas                   3.8.0           7_h6e990d7_netlib    conda-forge\r\n",
      "libcblas                  3.8.0           7_h6e990d7_netlib    conda-forge\r\n",
      "libedit                   3.1.20170329                  0    conda-forge\r\n",
      "libffi                    3.2.1                         3    conda-forge\r\n",
      "libgcc                    7.2.0                h69d50b8_2    conda-forge\r\n",
      "libgcc-ng                 7.3.0                hdf63c60_0    defaults\r\n",
      "libgfortran               3.0.0                         1    conda-forge\r\n",
      "libgfortran-ng            7.2.0                h9f7466a_2    defaults\r\n",
      "libgpuarray               0.7.5                         0    conda-forge\r\n",
      "libiconv                  1.15                 h470a237_1    conda-forge\r\n",
      "liblapack                 3.8.0           7_h6e990d7_netlib    conda-forge\r\n",
      "libopenblas               0.2.20               h9ac9557_4    defaults\r\n",
      "libpng                    1.6.34                        0    conda-forge\r\n",
      "libpq                     9.6.6                h1f21990_0    defaults\r\n",
      "librosa                   0.5.1                    py36_1    conda-forge\r\n",
      "libsodium                 1.0.15                        1    conda-forge\r\n",
      "libstdcxx-ng              7.3.0                hdf63c60_0    defaults\r\n",
      "libtiff                   4.0.9                he6b73bb_1    conda-forge\r\n",
      "libtorch                  0.1.12                  nomkl_0    defaults\r\n",
      "libuuid                   2.32.1            h14c3975_1000    conda-forge\r\n",
      "libwebp                   0.5.2                         7    conda-forge\r\n",
      "libxcb                    1.13                          0    conda-forge\r\n",
      "libxml2                   2.9.7                         0    conda-forge\r\n",
      "llvmlite                  0.20.0                   py36_0    defaults\r\n",
      "locket                    0.2.0                    py36_1    conda-forge\r\n",
      "lockfile                  0.12.2                   pypi_0    pypi\r\n",
      "lxml                      4.1.1                    pypi_0    pypi\r\n",
      "mako                      1.0.7                    pypi_0    pypi\r\n",
      "markdown                  2.6.9                    py36_0    defaults\r\n",
      "markupsafe                1.0                      py36_0    conda-forge\r\n",
      "matplotlib                2.1.0            py36hba5de38_0    defaults\r\n",
      "mistune                   0.8.3                      py_0    conda-forge\r\n",
      "mkl                       2018.0.1             h19d6760_4    defaults\r\n",
      "mock                      2.0.0                    py36_0    conda-forge\r\n",
      "monotonic                 1.3                      pypi_0    pypi\r\n",
      "more-itertools            7.0.0                    pypi_0    pypi\r\n",
      "moviepy                   0.2.3.2                  py36_0    conda-forge\r\n",
      "mpmath                    1.0.0                      py_0    conda-forge\r\n",
      "msgpack                   0.5.6                    pypi_0    pypi\r\n",
      "msgpack-python            0.4.8                    py36_0    conda-forge\r\n",
      "mujoco-py                 1.50.1.59                pypi_0    pypi\r\n",
      "nbconvert                 5.4.0                    py36_1    defaults\r\n",
      "nbformat                  4.4.0                    py36_0    conda-forge\r\n",
      "nbzip                     0.1.0                    pypi_0    pypi\r\n",
      "nccl                      1.3.4                 cuda8.0_1    defaults\r\n",
      "ncurses                   5.9                          10    conda-forge\r\n",
      "networkx                  1.11                     py36_0    conda-forge\r\n",
      "ninja                     1.8.2                         1    conda-forge\r\n",
      "nlohmann_json             3.4.0                hf484d3e_0    conda-forge\r\n",
      "nltk                      3.2.5                      py_0    conda-forge\r\n",
      "nomkl                     1.0                           0    defaults\r\n",
      "notebook                  5.7.0                    py36_0    conda-forge\r\n",
      "numba                     0.35.0              np112py36_0    defaults\r\n",
      "numexpr                   2.6.4           py36_nomklh2c28f9d_0    defaults\r\n",
      "numpy                     1.12.1          py36_blas_openblas_200  [blas_openblas]  conda-forge\r\n",
      "oauthlib                  2.0.6                      py_0    conda-forge\r\n",
      "olefile                   0.44                     py36_0    conda-forge\r\n",
      "openblas                  0.2.19                        2    conda-forge\r\n",
      "opencv                    3.3.1            py36h0f6f1c3_0    defaults\r\n",
      "openpyxl                  2.5.0b1                  py36_0    conda-forge\r\n",
      "openssl                   1.0.2u               h516909a_0    conda-forge\r\n",
      "packaging                 16.8                     py36_0    conda-forge\r\n",
      "pandas                    0.23.3                   py36_0    conda-forge\r\n",
      "pandoc                    1.19.2                        0    conda-forge\r\n",
      "pandocfilters             1.4.1                    py36_0    conda-forge\r\n",
      "parso                     0.4.0                      py_0    conda-forge\r\n",
      "partd                     0.3.8                    py36_0    conda-forge\r\n",
      "patsy                     0.4.1                    py36_0    conda-forge\r\n",
      "pbr                       3.1.1                    py36_0    conda-forge\r\n",
      "pcre                      8.39                          0    conda-forge\r\n",
      "pexpect                   4.3.1                    py36_0    conda-forge\r\n",
      "pickleshare               0.7.4                    py36_0    conda-forge\r\n",
      "pillow                    5.2.0                    py36_0    conda-forge\r\n",
      "pinyin                    0.4.0                    pypi_0    pypi\r\n",
      "pip                       18.1                     py36_0    defaults\r\n",
      "pixman                    0.34.0                        1    conda-forge\r\n",
      "plotly                    2.0.15                   py36_0    conda-forge\r\n",
      "pluggy                    0.11.0                   pypi_0    pypi\r\n",
      "pomegranate               0.9.0            py36h3010b51_0    defaults\r\n",
      "prettytable               0.7.2                      py_2    conda-forge\r\n",
      "prometheus_client         0.3.1                      py_1    conda-forge\r\n",
      "prompt_toolkit            1.0.15                   py36_0    conda-forge\r\n",
      "protobuf                  3.5.1                    py36_3    conda-forge\r\n",
      "psutil                    5.4.0                    py36_0    conda-forge\r\n",
      "psycopg2                  2.7.4            py36hb7f436b_0    defaults\r\n",
      "ptyprocess                0.5.2                    py36_0    conda-forge\r\n",
      "py                        1.8.0                    pypi_0    pypi\r\n",
      "py4j                      0.10.7                   py36_0    conda-forge\r\n",
      "pyasn1                    0.4.4                      py_1    conda-forge\r\n",
      "pycosat                   0.6.3                    py36_0    conda-forge\r\n",
      "pycparser                 2.18                     py36_0    conda-forge\r\n",
      "pycurl                    7.43.0                   py36_0    defaults\r\n",
      "pydot                     1.2.4                      py_0    conda-forge\r\n",
      "pydotplus                 2.0.2                    py36_0    conda-forge\r\n",
      "pyglet                    1.3.0                    pypi_0    pypi\r\n",
      "pygments                  2.2.0                    py36_0    conda-forge\r\n",
      "pygpu                     0.7.5                    py36_0    conda-forge\r\n",
      "pyjwt                     1.5.3                      py_0    conda-forge\r\n",
      "pymavlink                 2.2.20                   pypi_0    pypi\r\n",
      "pymc3                     3.2                      py36_0    conda-forge\r\n",
      "pyopengl                  3.1.0                    pypi_0    pypi\r\n",
      "pyopenssl                 17.4.0                   py36_0    conda-forge\r\n",
      "pyparsing                 2.2.0                    py36_0    conda-forge\r\n",
      "pyqt                      5.6.0                    py36_4    conda-forge\r\n",
      "pyrsistent                0.15.2           py36h516909a_0    conda-forge\r\n",
      "pysocks                   1.6.8                    py36_1    conda-forge\r\n",
      "pyspark                   2.4.3                      py_0    conda-forge\r\n",
      "pytest                    4.5.0                    pypi_0    pypi\r\n",
      "python                    3.6.3                         3    conda-forge\r\n",
      "python-crfsuite           0.9.2                    py36_0    conda-forge\r\n",
      "python-dateutil           2.6.1                    py36_0    conda-forge\r\n",
      "python-engineio           2.0.2                      py_0    conda-forge\r\n",
      "python-socketio           1.8.4                      py_0    conda-forge\r\n",
      "python-speech-features    0.6                      pypi_0    pypi\r\n",
      "pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n",
      "pytz                      2017.3                     py_2    conda-forge\r\n",
      "pywavelets                0.5.2                    py36_1    conda-forge\r\n",
      "pyyaml                    3.12                     py36_1    conda-forge\r\n",
      "pyzmq                     17.1.2           py36hae99301_0    conda-forge\r\n",
      "qt                        5.6.2                         7    conda-forge\r\n",
      "readline                  7.0                  hb321a52_4    defaults\r\n",
      "requests                  2.18.4                   py36_1    conda-forge\r\n",
      "requests-oauthlib         0.8.0                    py36_1    conda-forge\r\n",
      "requests-toolbelt         0.9.1                    pypi_0    pypi\r\n",
      "resampy                   0.2.0                    py36_1    conda-forge\r\n",
      "rsa                       3.4.2                      py_1    conda-forge\r\n",
      "ruamel_yaml               0.11.14                  py36_0    conda-forge\r\n",
      "s3transfer                0.1.13                   py36_0    conda-forge\r\n",
      "scikit-image              0.14.2           py36he6710b0_0    defaults\r\n",
      "scikit-learn              0.19.1          py36_blas_openblas_200  [blas_openblas]  conda-forge\r\n",
      "scipy                     1.2.1            py36h09a28d5_1    conda-forge\r\n",
      "seaborn                   0.8.1                    py36_0    conda-forge\r\n",
      "seekwell                  0.1                      pypi_0    pypi\r\n",
      "send2trash                1.5.0                      py_0    conda-forge\r\n",
      "setuptools                38.4.0                   py36_0    conda-forge\r\n",
      "shapely                   1.6.4                    py36_0    conda-forge\r\n",
      "simplegeneric             0.8.1                    py36_0    conda-forge\r\n",
      "simplewebsocketserver     0.1.1                    pypi_0    pypi\r\n",
      "sip                       4.18                     py36_1    conda-forge\r\n",
      "six                       1.11.0                   py36_1    conda-forge\r\n",
      "smart_open                1.5.6                    py36_1    conda-forge\r\n",
      "snownlp                   0.12.3                   pypi_0    pypi\r\n",
      "soundfile                 0.9.0.post1              pypi_0    pypi\r\n",
      "sqlalchemy                1.1.13                   py36_0    conda-forge\r\n",
      "sqlite                    3.20.1               h6d8b0f3_1    defaults\r\n",
      "sqlparse                  0.3.0                      py_0    conda-forge\r\n",
      "statsmodels               0.8.0                    py36_0    conda-forge\r\n",
      "sympy                     1.0                      py36_0    conda-forge\r\n",
      "tensorflow-gpu            1.3.0                         0    defaults\r\n",
      "tensorflow-gpu-base       1.3.0           py36cuda8.0cudnn6.0_1    defaults\r\n",
      "tensorflow-tensorboard    0.1.5                    py36_0    defaults\r\n",
      "terminado                 0.8.1                    py36_0    conda-forge\r\n",
      "testpath                  0.3.1                    py36_0    conda-forge\r\n",
      "theano                    1.0.1                    py36_1    conda-forge\r\n",
      "tk                        8.6.7                         0    conda-forge\r\n",
      "toolz                     0.8.2                      py_2    conda-forge\r\n",
      "torchvision               0.2.1                    py36_1    pytorch\r\n",
      "tornado                   4.5.3                    py36_0    conda-forge\r\n",
      "tqdm                      4.11.2                   py36_0    conda-forge\r\n",
      "traitlets                 4.3.2                    py36_0    conda-forge\r\n",
      "tweepy                    3.5.0                    py36_0    conda-forge\r\n",
      "twython                   3.6.0                    py36_0    conda-forge\r\n",
      "udacity-pa                0.2.9                    pypi_0    pypi\r\n",
      "urllib3                   1.22                     py36_0    conda-forge\r\n",
      "util-linux                2.21                          0    defaults\r\n",
      "utm                       0.4.2                    py36_0    conda-forge\r\n",
      "uvloop                    0.8.1                    py36_0    conda-forge\r\n",
      "vega                      0.4.4                    py36_1    conda-forge\r\n",
      "vincent                   0.4.4                    py36_0    conda-forge\r\n",
      "wcwidth                   0.1.7                    py36_0    conda-forge\r\n",
      "webencodings              0.5                      py36_0    conda-forge\r\n",
      "websockets                4.0.1                    py36_0    conda-forge\r\n",
      "werkzeug                  0.14.1                     py_0    conda-forge\r\n",
      "wheel                     0.30.0                   py36_2    conda-forge\r\n",
      "widgetsnbextension        3.1.0                    py36_0    conda-forge\r\n",
      "wordcloud                 1.4.1                    py36_0    conda-forge\r\n",
      "wptools                   0.4.7                    pypi_0    pypi\r\n",
      "x264                      20131217                      3    conda-forge\r\n",
      "xeus                      0.17.0            h6e680fc_1000    conda-forge\r\n",
      "xeus-cling                0.4.9                         0    quantstack\r\n",
      "xlrd                      1.1.0                      py_2    conda-forge\r\n",
      "xorg-libxau               1.0.8                         3    conda-forge\r\n",
      "xorg-libxdmcp             1.1.2                         3    conda-forge\r\n",
      "xtl                       0.5.4                h6bb024c_0    conda-forge\r\n",
      "xz                        5.2.3                         0    conda-forge\r\n",
      "yaml                      0.1.6                         0    conda-forge\r\n",
      "zeromq                    4.2.5                hfc679d8_4    conda-forge\r\n",
      "zlib                      1.2.11                        0    conda-forge\r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La data corresponde a un archivo txt que contiene diálogos de la serie Senfield. Primero que todo cargamos la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "input_file = os.path.join(data_dir)\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos las Funciones Para Generar Los Diccionarios y las Puntuaciones\n",
    "\n",
    "### Generando los Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab_to_int = dict()\n",
    "    int_to_vocab = dict()\n",
    "    \n",
    "    id = 0\n",
    "    for i in text:\n",
    "        if i not in vocab_to_int.keys():\n",
    "            vocab_to_int[i] = id\n",
    "            id += 1\n",
    "            \n",
    "    for i,c in enumerate(vocab_to_int.keys()):\n",
    "        int_to_vocab[i] = c        \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazando Las Puntuaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    symbols = ['.', ',', '\"', ';', '!', '?', '(', ')', '-', '\\n']\n",
    "    token = [\"||period||\" , \"||comma||\" , \"||quotationmark||\" , \"||semicolon||\" , \"||exclamation_mark||\" , \"||question_mark||\" ,\n",
    "            \"||left_parentheses||\" , \"||right_parentheses||\" , \"||dash||\" , \"||return||\"]\n",
    "    return dict(zip(symbols, token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora Preprocesamos Todo el Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "text = data\n",
    "\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]\n",
    "\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "int_text = [vocab_to_int[word] for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo la Red\n",
    "\n",
    "### Verificamos Acceso a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array([1, 2, 3, 4, 5, 6, 7, 8 , 9 , 10 , 11 , 12 , 13])\n",
    "sequence_length = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando los Batches : Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text : [ 1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "Sequence 1 : [1 2 3 4] Target : 5\n",
      "Sequence 2 : [2 3 4 5] Target : 6\n",
      "Sequence 3 : [3 4 5 6] Target : 7\n",
      "Sequence 4 : [4 5 6 7] Target : 8\n",
      "Sequence 5 : [5 6 7 8] Target : 9\n",
      "Sequence 6 : [6 7 8 9] Target : 10\n",
      "Sequence 7 : [ 7  8  9 10] Target : 11\n",
      "Sequence 8 : [ 8  9 10 11] Target : 12\n",
      "Sequence 9 : [ 9 10 11 12] Target : 13\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "targets = []\n",
    "print(f\"Sample Text : {words}\")\n",
    "for i in range(len(words) - sequence_length):\n",
    "    features.append(words[i : i+sequence_length])\n",
    "    targets.append(words[i+sequence_length])\n",
    "    print(f\"Sequence {i+1} : {words[i : i+sequence_length]} Target : {words[i+sequence_length]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    maxindex = len(words) - sequence_length\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        features.append(words[i : i+sequence_length])\n",
    "        targets.append(words[i+sequence_length])\n",
    "    dataset = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array((targets))))\n",
    "    data_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando el Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 42,  43,  44,  45,  46],\n",
      "        [ 38,  39,  40,  41,  42],\n",
      "        [ 39,  40,  41,  42,  43],\n",
      "        [ 21,  22,  23,  24,  25],\n",
      "        [ 36,  37,  38,  39,  40],\n",
      "        [ 23,  24,  25,  26,  27],\n",
      "        [ 10,  11,  12,  13,  14],\n",
      "        [ 35,  36,  37,  38,  39],\n",
      "        [ 13,  14,  15,  16,  17],\n",
      "        [  6,   7,   8,   9,  10]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 47,  43,  44,  26,  41,  28,  15,  40,  18,  11])\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo la Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que hay algunos __Print__ que están comentados, lo anterior para el que quiera ver los outputs línea por línea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\" \n",
    "        batch_size = nn_input.size(0)\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1] \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos Propagación Hacia Delante, Hacia Atrás y Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()            \n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "\n",
    "    rnn.zero_grad()   \n",
    "\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    loss = criterion(output, target.long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 25\n",
    "# Learning Rate\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 300\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25 epoch(s)...\n",
      "Epoch:    1/25    Loss: 5.290066518783569\n",
      "\n",
      "Epoch:    1/25    Loss: 4.728915277719498\n",
      "\n",
      "Epoch:    1/25    Loss: 4.551168636798859\n",
      "\n",
      "Epoch:    1/25    Loss: 4.459713696479797\n",
      "\n",
      "Epoch:    1/25    Loss: 4.347080304145813\n",
      "\n",
      "Epoch:    1/25    Loss: 4.311201413631439\n",
      "\n",
      "Epoch:    1/25    Loss: 4.279906487464904\n",
      "\n",
      "Epoch:    1/25    Loss: 4.228494143486023\n",
      "\n",
      "Epoch:    1/25    Loss: 4.222282981157303\n",
      "\n",
      "Epoch:    1/25    Loss: 4.191772379875183\n",
      "\n",
      "Epoch:    1/25    Loss: 4.144320255041122\n",
      "\n",
      "Epoch:    1/25    Loss: 4.169014077425003\n",
      "\n",
      "Epoch:    1/25    Loss: 4.15263531255722\n",
      "\n",
      "Epoch:    2/25    Loss: 4.021310191476634\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9547448177337645\n",
      "\n",
      "Epoch:    2/25    Loss: 3.955145547866821\n",
      "\n",
      "Epoch:    2/25    Loss: 3.935763961315155\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9508822515010835\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9760567026138305\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9530919563770293\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9880911026000976\n",
      "\n",
      "Epoch:    2/25    Loss: 3.968250494480133\n",
      "\n",
      "Epoch:    2/25    Loss: 3.9682933123111725\n",
      "\n",
      "Epoch:    2/25    Loss: 3.958000526189804\n",
      "\n",
      "Epoch:    2/25    Loss: 3.952339148044586\n",
      "\n",
      "Epoch:    2/25    Loss: 3.96004753613472\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8652927441471565\n",
      "\n",
      "Epoch:    3/25    Loss: 3.7710254249572754\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8047087018489836\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8209086892604827\n",
      "\n",
      "Epoch:    3/25    Loss: 3.794334720849991\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8137051799297335\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8305844748020172\n",
      "\n",
      "Epoch:    3/25    Loss: 3.870702449321747\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8722127532958983\n",
      "\n",
      "Epoch:    3/25    Loss: 3.848176880121231\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8637433125972747\n",
      "\n",
      "Epoch:    3/25    Loss: 3.8695048274993895\n",
      "\n",
      "Epoch:    3/25    Loss: 3.854189116716385\n",
      "\n",
      "Epoch:    4/25    Loss: 3.7895172941985975\n",
      "\n",
      "Epoch:    4/25    Loss: 3.702086949825287\n",
      "\n",
      "Epoch:    4/25    Loss: 3.716994937419891\n",
      "\n",
      "Epoch:    4/25    Loss: 3.714196864128113\n",
      "\n",
      "Epoch:    4/25    Loss: 3.7355055701732636\n",
      "\n",
      "Epoch:    4/25    Loss: 3.732941109895706\n",
      "\n",
      "Epoch:    4/25    Loss: 3.7662751097679137\n",
      "\n",
      "Epoch:    4/25    Loss: 3.776364386320114\n",
      "\n",
      "Epoch:    4/25    Loss: 3.7879900834560396\n",
      "\n",
      "Epoch:    4/25    Loss: 3.7804398543834687\n",
      "\n",
      "Epoch:    4/25    Loss: 3.794419043302536\n",
      "\n",
      "Epoch:    4/25    Loss: 3.8007067170143127\n",
      "\n",
      "Epoch:    4/25    Loss: 3.789521227121353\n",
      "\n",
      "Epoch:    5/25    Loss: 3.7270365770850495\n",
      "\n",
      "Epoch:    5/25    Loss: 3.633755248308182\n",
      "\n",
      "Epoch:    5/25    Loss: 3.6526569039821624\n",
      "\n",
      "Epoch:    5/25    Loss: 3.6710696604251862\n",
      "\n",
      "Epoch:    5/25    Loss: 3.6658842761516572\n",
      "\n",
      "Epoch:    5/25    Loss: 3.6812456829547884\n",
      "\n",
      "Epoch:    5/25    Loss: 3.6864862616062166\n",
      "\n",
      "Epoch:    5/25    Loss: 3.7097945132255554\n",
      "\n",
      "Epoch:    5/25    Loss: 3.7238050122261046\n",
      "\n",
      "Epoch:    5/25    Loss: 3.7388286530971526\n",
      "\n",
      "Epoch:    5/25    Loss: 3.742988749742508\n",
      "\n",
      "Epoch:    5/25    Loss: 3.7360266513824465\n",
      "\n",
      "Epoch:    5/25    Loss: 3.766993423700333\n",
      "\n",
      "Epoch:    6/25    Loss: 3.675622922726179\n",
      "\n",
      "Epoch:    6/25    Loss: 3.622439017057419\n",
      "\n",
      "Epoch:    6/25    Loss: 3.5956056582927705\n",
      "\n",
      "Epoch:    6/25    Loss: 3.6242091767787934\n",
      "\n",
      "Epoch:    6/25    Loss: 3.6249949338436127\n",
      "\n",
      "Epoch:    6/25    Loss: 3.655171177148819\n",
      "\n",
      "Epoch:    6/25    Loss: 3.6530003855228426\n",
      "\n",
      "Epoch:    6/25    Loss: 3.670165440559387\n",
      "\n",
      "Epoch:    6/25    Loss: 3.6701892004013064\n",
      "\n",
      "Epoch:    6/25    Loss: 3.6870534613132477\n",
      "\n",
      "Epoch:    6/25    Loss: 3.7046338019371032\n",
      "\n",
      "Epoch:    6/25    Loss: 3.705387038707733\n",
      "\n",
      "Epoch:    6/25    Loss: 3.7127878334522246\n",
      "\n",
      "Epoch:    7/25    Loss: 3.6334694298719117\n",
      "\n",
      "Epoch:    7/25    Loss: 3.560830934524536\n",
      "\n",
      "Epoch:    7/25    Loss: 3.573151153087616\n",
      "\n",
      "Epoch:    7/25    Loss: 3.590437107801437\n",
      "\n",
      "Epoch:    7/25    Loss: 3.587594038486481\n",
      "\n",
      "Epoch:    7/25    Loss: 3.6166108531951906\n",
      "\n",
      "Epoch:    7/25    Loss: 3.6057406797409057\n",
      "\n",
      "Epoch:    7/25    Loss: 3.6341326379776\n",
      "\n",
      "Epoch:    7/25    Loss: 3.647270930051804\n",
      "\n",
      "Epoch:    7/25    Loss: 3.6474201924800873\n",
      "\n",
      "Epoch:    7/25    Loss: 3.658347322702408\n",
      "\n",
      "Epoch:    7/25    Loss: 3.7071935169696806\n",
      "\n",
      "Epoch:    7/25    Loss: 3.702264937400818\n",
      "\n",
      "Epoch:    8/25    Loss: 3.610497090052919\n",
      "\n",
      "Epoch:    8/25    Loss: 3.542019033908844\n",
      "\n",
      "Epoch:    8/25    Loss: 3.549912510633469\n",
      "\n",
      "Epoch:    8/25    Loss: 3.5595974555015566\n",
      "\n",
      "Epoch:    8/25    Loss: 3.551686649799347\n",
      "\n",
      "Epoch:    8/25    Loss: 3.590179712533951\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6057501924037934\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6152228453159334\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6264407267570498\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6325629260540007\n",
      "\n",
      "Epoch:    8/25    Loss: 3.620398215532303\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6637406558990477\n",
      "\n",
      "Epoch:    8/25    Loss: 3.6637848076820374\n",
      "\n",
      "Epoch:    9/25    Loss: 3.5888377271281375\n",
      "\n",
      "Epoch:    9/25    Loss: 3.506153328895569\n",
      "\n",
      "Epoch:    9/25    Loss: 3.51036070227623\n",
      "\n",
      "Epoch:    9/25    Loss: 3.5386242117881777\n",
      "\n",
      "Epoch:    9/25    Loss: 3.5530566694736483\n",
      "\n",
      "Epoch:    9/25    Loss: 3.5535516262054445\n",
      "\n",
      "Epoch:    9/25    Loss: 3.563128628730774\n",
      "\n",
      "Epoch:    9/25    Loss: 3.603472682952881\n",
      "\n",
      "Epoch:    9/25    Loss: 3.581917735338211\n",
      "\n",
      "Epoch:    9/25    Loss: 3.622372950553894\n",
      "\n",
      "Epoch:    9/25    Loss: 3.6430110578536987\n",
      "\n",
      "Epoch:    9/25    Loss: 3.6454452753067015\n",
      "\n",
      "Epoch:    9/25    Loss: 3.6442625069618226\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5577995134051648\n",
      "\n",
      "Epoch:   10/25    Loss: 3.4733754298686983\n",
      "\n",
      "Epoch:   10/25    Loss: 3.508262701511383\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5099206626415254\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5136927971839906\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5709524686336516\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5517367947101595\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5709253721237184\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5777706990242004\n",
      "\n",
      "Epoch:   10/25    Loss: 3.5861686952114105\n",
      "\n",
      "Epoch:   10/25    Loss: 3.610598462820053\n",
      "\n",
      "Epoch:   10/25    Loss: 3.6210755562782286\n",
      "\n",
      "Epoch:   10/25    Loss: 3.6295112764835356\n",
      "\n",
      "Epoch:   11/25    Loss: 3.548251705233617\n",
      "\n",
      "Epoch:   11/25    Loss: 3.473477153778076\n",
      "\n",
      "Epoch:   11/25    Loss: 3.471754449129105\n",
      "\n",
      "Epoch:   11/25    Loss: 3.506251830816269\n",
      "\n",
      "Epoch:   11/25    Loss: 3.535847449541092\n",
      "\n",
      "Epoch:   11/25    Loss: 3.496802489042282\n",
      "\n",
      "Epoch:   11/25    Loss: 3.519062822818756\n",
      "\n",
      "Epoch:   11/25    Loss: 3.5601216309070587\n",
      "\n",
      "Epoch:   11/25    Loss: 3.555859477519989\n",
      "\n",
      "Epoch:   11/25    Loss: 3.5656760687828064\n",
      "\n",
      "Epoch:   11/25    Loss: 3.586207985639572\n",
      "\n",
      "Epoch:   11/25    Loss: 3.598790401697159\n",
      "\n",
      "Epoch:   11/25    Loss: 3.60098033618927\n",
      "\n",
      "Epoch:   12/25    Loss: 3.5199494770103166\n",
      "\n",
      "Epoch:   12/25    Loss: 3.4563702943325043\n",
      "\n",
      "Epoch:   12/25    Loss: 3.4648668880462647\n",
      "\n",
      "Epoch:   12/25    Loss: 3.4813552720546723\n",
      "\n",
      "Epoch:   12/25    Loss: 3.4929227697849274\n",
      "\n",
      "Epoch:   12/25    Loss: 3.4871947212219236\n",
      "\n",
      "Epoch:   12/25    Loss: 3.5286793541908263\n",
      "\n",
      "Epoch:   12/25    Loss: 3.526876791715622\n",
      "\n",
      "Epoch:   12/25    Loss: 3.542092357635498\n",
      "\n",
      "Epoch:   12/25    Loss: 3.545739206314087\n",
      "\n",
      "Epoch:   12/25    Loss: 3.5676459205150604\n",
      "\n",
      "Epoch:   12/25    Loss: 3.5788700895309447\n",
      "\n",
      "Epoch:   12/25    Loss: 3.5918351159095763\n",
      "\n",
      "Epoch:   13/25    Loss: 3.505541934617836\n",
      "\n",
      "Epoch:   13/25    Loss: 3.4434149606227873\n",
      "\n",
      "Epoch:   13/25    Loss: 3.444742538690567\n",
      "\n",
      "Epoch:   13/25    Loss: 3.4534187479019165\n",
      "\n",
      "Epoch:   13/25    Loss: 3.4817224452495577\n",
      "\n",
      "Epoch:   13/25    Loss: 3.500231281518936\n",
      "\n",
      "Epoch:   13/25    Loss: 3.497187618494034\n",
      "\n",
      "Epoch:   13/25    Loss: 3.497184892177582\n",
      "\n",
      "Epoch:   13/25    Loss: 3.523425679922104\n",
      "\n",
      "Epoch:   13/25    Loss: 3.547571886062622\n",
      "\n",
      "Epoch:   13/25    Loss: 3.5419442920684814\n",
      "\n",
      "Epoch:   13/25    Loss: 3.5641623723506926\n",
      "\n",
      "Epoch:   13/25    Loss: 3.5682927107810976\n",
      "\n",
      "Epoch:   14/25    Loss: 3.4832279166958884\n",
      "\n",
      "Epoch:   14/25    Loss: 3.4163189327716825\n",
      "\n",
      "Epoch:   14/25    Loss: 3.429474548101425\n",
      "\n",
      "Epoch:   14/25    Loss: 3.447964967727661\n",
      "\n",
      "Epoch:   14/25    Loss: 3.4565256617069244\n",
      "\n",
      "Epoch:   14/25    Loss: 3.4739374270439147\n",
      "\n",
      "Epoch:   14/25    Loss: 3.490132453203201\n",
      "\n",
      "Epoch:   14/25    Loss: 3.483453584909439\n",
      "\n",
      "Epoch:   14/25    Loss: 3.508994609117508\n",
      "\n",
      "Epoch:   14/25    Loss: 3.5338541219234467\n",
      "\n",
      "Epoch:   14/25    Loss: 3.5269306876659394\n",
      "\n",
      "Epoch:   14/25    Loss: 3.5397494750022886\n",
      "\n",
      "Epoch:   14/25    Loss: 3.572752272605896\n",
      "\n",
      "Epoch:   15/25    Loss: 3.4766327142100657\n",
      "\n",
      "Epoch:   15/25    Loss: 3.4117100207805633\n",
      "\n",
      "Epoch:   15/25    Loss: 3.40730814743042\n",
      "\n",
      "Epoch:   15/25    Loss: 3.437022704362869\n",
      "\n",
      "Epoch:   15/25    Loss: 3.4345151119232176\n",
      "\n",
      "Epoch:   15/25    Loss: 3.459622162103653\n",
      "\n",
      "Epoch:   15/25    Loss: 3.4710117440223693\n",
      "\n",
      "Epoch:   15/25    Loss: 3.480635678768158\n",
      "\n",
      "Epoch:   15/25    Loss: 3.497441708087921\n",
      "\n",
      "Epoch:   15/25    Loss: 3.508045014858246\n",
      "\n",
      "Epoch:   15/25    Loss: 3.516853548288345\n",
      "\n",
      "Epoch:   15/25    Loss: 3.5309488599300383\n",
      "\n",
      "Epoch:   15/25    Loss: 3.5477203817367555\n",
      "\n",
      "Epoch:   16/25    Loss: 3.463443858159947\n",
      "\n",
      "Epoch:   16/25    Loss: 3.392502538204193\n",
      "\n",
      "Epoch:   16/25    Loss: 3.4159760689735412\n",
      "\n",
      "Epoch:   16/25    Loss: 3.4176586334705354\n",
      "\n",
      "Epoch:   16/25    Loss: 3.4185949771404265\n",
      "\n",
      "Epoch:   16/25    Loss: 3.444460525274277\n",
      "\n",
      "Epoch:   16/25    Loss: 3.4546881392002105\n",
      "\n",
      "Epoch:   16/25    Loss: 3.4898428750038146\n",
      "\n",
      "Epoch:   16/25    Loss: 3.493128385066986\n",
      "\n",
      "Epoch:   16/25    Loss: 3.477689263343811\n",
      "\n",
      "Epoch:   16/25    Loss: 3.5108015248775484\n",
      "\n",
      "Epoch:   16/25    Loss: 3.512824632167816\n",
      "\n",
      "Epoch:   16/25    Loss: 3.5377185196876524\n",
      "\n",
      "Epoch:   17/25    Loss: 3.447706602356248\n",
      "\n",
      "Epoch:   17/25    Loss: 3.3894956929683686\n",
      "\n",
      "Epoch:   17/25    Loss: 3.401965343952179\n",
      "\n",
      "Epoch:   17/25    Loss: 3.405473140716553\n",
      "\n",
      "Epoch:   17/25    Loss: 3.4298071084022523\n",
      "\n",
      "Epoch:   17/25    Loss: 3.430460762023926\n",
      "\n",
      "Epoch:   17/25    Loss: 3.469612271308899\n",
      "\n",
      "Epoch:   17/25    Loss: 3.4559175953865053\n",
      "\n",
      "Epoch:   17/25    Loss: 3.4722151968479156\n",
      "\n",
      "Epoch:   17/25    Loss: 3.504662648677826\n",
      "\n",
      "Epoch:   17/25    Loss: 3.488853966474533\n",
      "\n",
      "Epoch:   17/25    Loss: 3.4958866016864776\n",
      "\n",
      "Epoch:   17/25    Loss: 3.510638422250748\n",
      "\n",
      "Epoch:   18/25    Loss: 3.440153292001308\n",
      "\n",
      "Epoch:   18/25    Loss: 3.36956263422966\n",
      "\n",
      "Epoch:   18/25    Loss: 3.3770482676029205\n",
      "\n",
      "Epoch:   18/25    Loss: 3.3980838963985445\n",
      "\n",
      "Epoch:   18/25    Loss: 3.4095021238327026\n",
      "\n",
      "Epoch:   18/25    Loss: 3.417017389535904\n",
      "\n",
      "Epoch:   18/25    Loss: 3.423779623746872\n",
      "\n",
      "Epoch:   18/25    Loss: 3.4552302749156953\n",
      "\n",
      "Epoch:   18/25    Loss: 3.4631026942729948\n",
      "\n",
      "Epoch:   18/25    Loss: 3.4639851729869844\n",
      "\n",
      "Epoch:   18/25    Loss: 3.510652357816696\n",
      "\n",
      "Epoch:   18/25    Loss: 3.5021476209163667\n",
      "\n",
      "Epoch:   18/25    Loss: 3.5023300054073334\n",
      "\n",
      "Epoch:   19/25    Loss: 3.4456407315587185\n",
      "\n",
      "Epoch:   19/25    Loss: 3.371037771940231\n",
      "\n",
      "Epoch:   19/25    Loss: 3.3891580502986907\n",
      "\n",
      "Epoch:   19/25    Loss: 3.3564111649990083\n",
      "\n",
      "Epoch:   19/25    Loss: 3.408308823108673\n",
      "\n",
      "Epoch:   19/25    Loss: 3.4260643093585967\n",
      "\n",
      "Epoch:   19/25    Loss: 3.425722579240799\n",
      "\n",
      "Epoch:   19/25    Loss: 3.442954017877579\n",
      "\n",
      "Epoch:   19/25    Loss: 3.461973871707916\n",
      "\n",
      "Epoch:   19/25    Loss: 3.4779704983234407\n",
      "\n",
      "Epoch:   19/25    Loss: 3.476485858440399\n",
      "\n",
      "Epoch:   19/25    Loss: 3.4973001465797426\n",
      "\n",
      "Epoch:   19/25    Loss: 3.491580050468445\n",
      "\n",
      "Epoch:   20/25    Loss: 3.434641994724205\n",
      "\n",
      "Epoch:   20/25    Loss: 3.367121566772461\n",
      "\n",
      "Epoch:   20/25    Loss: 3.381658193588257\n",
      "\n",
      "Epoch:   20/25    Loss: 3.3810282022953033\n",
      "\n",
      "Epoch:   20/25    Loss: 3.3870537195205688\n",
      "\n",
      "Epoch:   20/25    Loss: 3.4510223593711853\n",
      "\n",
      "Epoch:   20/25    Loss: 3.420708796739578\n",
      "\n",
      "Epoch:   20/25    Loss: 3.433892320871353\n",
      "\n",
      "Epoch:   20/25    Loss: 3.4555661356449128\n",
      "\n",
      "Epoch:   20/25    Loss: 3.451907483100891\n",
      "\n",
      "Epoch:   20/25    Loss: 3.4625268869400023\n",
      "\n",
      "Epoch:   20/25    Loss: 3.484656124830246\n",
      "\n",
      "Epoch:   20/25    Loss: 3.4761334903240204\n",
      "\n",
      "Epoch:   21/25    Loss: 3.425561399420492\n",
      "\n",
      "Epoch:   21/25    Loss: 3.3419704105854033\n",
      "\n",
      "Epoch:   21/25    Loss: 3.3618116054534912\n",
      "\n",
      "Epoch:   21/25    Loss: 3.386962708234787\n",
      "\n",
      "Epoch:   21/25    Loss: 3.396248323202133\n",
      "\n",
      "Epoch:   21/25    Loss: 3.4087853784561157\n",
      "\n",
      "Epoch:   21/25    Loss: 3.4197408607006072\n",
      "\n",
      "Epoch:   21/25    Loss: 3.428329031944275\n",
      "\n",
      "Epoch:   21/25    Loss: 3.429322226524353\n",
      "\n",
      "Epoch:   21/25    Loss: 3.4604111750125885\n",
      "\n",
      "Epoch:   21/25    Loss: 3.47297536277771\n",
      "\n",
      "Epoch:   21/25    Loss: 3.47439962720871\n",
      "\n",
      "Epoch:   21/25    Loss: 3.4795819091796876\n",
      "\n",
      "Epoch:   22/25    Loss: 3.411335761919656\n",
      "\n",
      "Epoch:   22/25    Loss: 3.3523984210491182\n",
      "\n",
      "Epoch:   22/25    Loss: 3.3618706424236295\n",
      "\n",
      "Epoch:   22/25    Loss: 3.3707186901569366\n",
      "\n",
      "Epoch:   22/25    Loss: 3.386233899831772\n",
      "\n",
      "Epoch:   22/25    Loss: 3.4135390105247496\n",
      "\n",
      "Epoch:   22/25    Loss: 3.4110332424640655\n",
      "\n",
      "Epoch:   22/25    Loss: 3.417397741317749\n",
      "\n",
      "Epoch:   22/25    Loss: 3.421923980951309\n",
      "\n",
      "Epoch:   22/25    Loss: 3.4433325612545014\n",
      "\n",
      "Epoch:   22/25    Loss: 3.4546126132011414\n",
      "\n",
      "Epoch:   22/25    Loss: 3.47652409863472\n",
      "\n",
      "Epoch:   22/25    Loss: 3.4655083754062654\n",
      "\n",
      "Epoch:   23/25    Loss: 3.403567498095445\n",
      "\n",
      "Epoch:   23/25    Loss: 3.3403953952789305\n",
      "\n",
      "Epoch:   23/25    Loss: 3.3677289066314695\n",
      "\n",
      "Epoch:   23/25    Loss: 3.364497171878815\n",
      "\n",
      "Epoch:   23/25    Loss: 3.3696344435214995\n",
      "\n",
      "Epoch:   23/25    Loss: 3.3839118258953094\n",
      "\n",
      "Epoch:   23/25    Loss: 3.420358062505722\n",
      "\n",
      "Epoch:   23/25    Loss: 3.410602998018265\n",
      "\n",
      "Epoch:   23/25    Loss: 3.403846896648407\n",
      "\n",
      "Epoch:   23/25    Loss: 3.439890475988388\n",
      "\n",
      "Epoch:   23/25    Loss: 3.4401537067890167\n",
      "\n",
      "Epoch:   23/25    Loss: 3.4632130467891695\n",
      "\n",
      "Epoch:   23/25    Loss: 3.470803027868271\n",
      "\n",
      "Epoch:   24/25    Loss: 3.381753661203409\n",
      "\n",
      "Epoch:   24/25    Loss: 3.344273595094681\n",
      "\n",
      "Epoch:   24/25    Loss: 3.3419045021533966\n",
      "\n",
      "Epoch:   24/25    Loss: 3.36289594244957\n",
      "\n",
      "Epoch:   24/25    Loss: 3.362986681699753\n",
      "\n",
      "Epoch:   24/25    Loss: 3.394638212919235\n",
      "\n",
      "Epoch:   24/25    Loss: 3.4210880448818206\n",
      "\n",
      "Epoch:   24/25    Loss: 3.3841700456142427\n",
      "\n",
      "Epoch:   24/25    Loss: 3.4183522851467134\n",
      "\n",
      "Epoch:   24/25    Loss: 3.419933331489563\n",
      "\n",
      "Epoch:   24/25    Loss: 3.4554603102207184\n",
      "\n",
      "Epoch:   24/25    Loss: 3.436114850997925\n",
      "\n",
      "Epoch:   24/25    Loss: 3.4508374526500702\n",
      "\n",
      "Epoch:   25/25    Loss: 3.393765100441746\n",
      "\n",
      "Epoch:   25/25    Loss: 3.3282671885490416\n",
      "\n",
      "Epoch:   25/25    Loss: 3.3335695366859435\n",
      "\n",
      "Epoch:   25/25    Loss: 3.3397728402614595\n",
      "\n",
      "Epoch:   25/25    Loss: 3.3699771757125854\n",
      "\n",
      "Epoch:   25/25    Loss: 3.374299437046051\n",
      "\n",
      "Epoch:   25/25    Loss: 3.3884012167453768\n",
      "\n",
      "Epoch:   25/25    Loss: 3.4130576541423796\n",
      "\n",
      "Epoch:   25/25    Loss: 3.4171784212589262\n",
      "\n",
      "Epoch:   25/25    Loss: 3.4325679495334627\n",
      "\n",
      "Epoch:   25/25    Loss: 3.437373152017593\n",
      "\n",
      "Epoch:   25/25    Loss: 3.4361917657852175\n",
      "\n",
      "Epoch:   25/25    Loss: 3.4478311915397644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():\n",
    "    # create model and move to gpu if available\n",
    "    rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "    if train_on_gpu:\n",
    "        rnn.cuda()\n",
    "\n",
    "    # defining loss and optimization functions for training\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training the model\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "    # saving the trained model\n",
    "    helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess2.p', mode='rb'))\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando Diálogos : Versión Corta Explicativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "#     print(f\"Initial Word Token for jerry: {prime_id}\")\n",
    "#     print(f\"Padding Token : {pad_value}\")\n",
    "#     print(f\"Padding Word : {int_to_vocab[pad_value]}\")\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "#     print(f\"This is the Initial Empty Sequence {current_seq}\")\n",
    "    current_seq[-1][-1] = prime_id\n",
    "#     print(f\"We set the initial token at the end : {current_seq}\")\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "#     print(f\"Initial Predicted Word : {predicted}\")\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "\n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        #print(f\"The output is length of vocabulary : {output.shape}\")\n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        #print(f\"We apply softmax to convert to probability : {p.shape}\")\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        #print(f\"After obtaining the probabilities we choose the top 5. We only care about the index : {top_i}\")\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        #print(f\"We add Randomness on the top 5 words : {word_i} \")\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)\n",
    "        #print(f\"We then search the selected word and transform it into text. Finally we append to predited : {predicted}\")\n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "        #print(f\"New Current Sequence : {current_seq}\\n\")\n",
    "       #print(\"===========END OF PREDICTION, STARTING NEW PREDICTION===================\\n\")\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Word Token for jerry: 101\n",
      "Padding Token : 21387\n",
      "Padding Word : <PAD>\n",
      "This is the Initial Empty Sequence [[21387 21387 21387 21387]]\n",
      "We set the initial token at the end : [[21387 21387 21387   101]]\n",
      "Initial Predicted Word : ['jerry:']\n",
      "The output is length of vocabulary : torch.Size([1, 21388])\n",
      "We apply softmax to convert to probability : torch.Size([1, 21388])\n",
      "After obtaining the probabilities we choose the top 5. We only care about the index : tensor([[  3611,   2570,   5354,   7734,  13346]])\n",
      "We add Randomness on the top 5 words : 5354 \n",
      "We then search the selected word and transform it into text. Finally we append to predited : ['jerry:', 'community']\n",
      "New Current Sequence : [[21387 21387   101  5354]]\n",
      "\n",
      "===========END OF PREDICTION, STARTING NEW PREDICTION===================\n",
      "\n",
      "The output is length of vocabulary : torch.Size([1, 21388])\n",
      "We apply softmax to convert to probability : torch.Size([1, 21388])\n",
      "After obtaining the probabilities we choose the top 5. We only care about the index : tensor([[     3,  13147,    568,   3994,  10005]])\n",
      "We add Randomness on the top 5 words : 13147 \n",
      "We then search the selected word and transform it into text. Finally we append to predited : ['jerry:', 'community', 'knob']\n",
      "New Current Sequence : [[21387   101  5354 13147]]\n",
      "\n",
      "===========END OF PREDICTION, STARTING NEW PREDICTION===================\n",
      "\n",
      "The output is length of vocabulary : torch.Size([1, 21388])\n",
      "We apply softmax to convert to probability : torch.Size([1, 21388])\n",
      "After obtaining the probabilities we choose the top 5. We only care about the index : tensor([[  3,  28,  23,  37,  41]])\n",
      "We add Randomness on the top 5 words : 3 \n",
      "We then search the selected word and transform it into text. Finally we append to predited : ['jerry:', 'community', 'knob', '||period||']\n",
      "New Current Sequence : [[  101  5354 13147     3]]\n",
      "\n",
      "===========END OF PREDICTION, STARTING NEW PREDICTION===================\n",
      "\n",
      "jerry: community knob.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 3 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando Diálogos: Versión Extensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: saving bills.\n",
      "\n",
      "kramer: hey, what do you mean, uh, i have to get rid of the way of the world series of spring.\n",
      "\n",
      "jerry: you know i don't want to get a massage?\n",
      "\n",
      "elaine: oh, yeah!(to jerry and elaine and i agreed to go to the hospital, you know...\n",
      "\n",
      "jerry: oh, i forgot to get rid of her. i mean, i'm gonna go get a chance, and i was wondering.\n",
      "\n",
      "elaine: oh, yeah, yeah.\n",
      "\n",
      "kramer: oh, you gotta be able to be a clown...\n",
      "\n",
      "kramer: oh, you got a little more flexible.\n",
      "\n",
      "george: you know...\n",
      "\n",
      "jerry: yeah, well, it's all emotional, and rich, and rich.\n",
      "\n",
      "jerry:(looking at george) oh, i don't know, i'm afraid i can get you another one..\n",
      "\n",
      "jerry: i mean, i can't believe this.\n",
      "\n",
      "elaine: oh, yeah.(takes the pillow back to the freeze.\n",
      "\n",
      "george: yeah, i got a little distracted, jerry, he doesn't deserve it.\n",
      "\n",
      "george:(to kramer) hey, jerry.\n",
      "\n",
      "kramer: oh, yeah, sure.\n",
      "\n",
      "kramer: oh, i was wondering. you have a big salad.\n",
      "\n",
      "jerry: yeah!!\n",
      "\n",
      "kramer: well, i don't know what i think.\n",
      "\n",
      "kramer: hey, i saw you whenever it gets alive, i don't want to see a doctor.\n",
      "\n",
      "george:(pointing to the crowd, i mean...\n",
      "\n",
      "kramer: oh, well, you know, i can't find it.\n",
      "\n",
      "jerry: i know.(he opens his head.)\n",
      "\n",
      "jerry: what did he say.\n",
      "\n",
      "elaine:(to jay) isnt that 555- fat, and none of you?\n",
      "\n",
      "jerry:(pause)\n",
      "\n",
      "jerry: i think you're wrong. i don't know how much duty is. i can't. i'm not really sorry to\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
