{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb9847a",
   "metadata": {},
   "source": [
    "# Entendiendo las RNNs by Marcelo Torres Cisterna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06c62a",
   "metadata": {},
   "source": [
    "Primero comenzamos importando los módulos necesarios. Para el ejemplo en cuestión utilizaré __PyTorch__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5791132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d9c28",
   "metadata": {},
   "source": [
    "![RNN](rnncell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43380e62",
   "metadata": {},
   "source": [
    "La unidad básica de una __Red Neuronal Recurrente__ se muestra en el apartado anterior. Al comienzo puede parecer bastante confuso ya que existen una serie de operaciones utilizadas para crear tanto el __Hidden State__ como el output. En las siguientes líneas iremos deglosando paso a paso las operaciones matemáticas que se muestran en la figura.\n",
    "\n",
    "Podemos apreciar que el input tiene un índice __t__ y ésto es porque las RNN reciben __secuencias__ de datos como por ejemplo los valores en el tiempo de una acción o frases. Cada input de la secuencia a su vez genera un output para ese input y un estado denominado, __Hidden State__ que se utiliza en el siguiente input de la secuencia. La función de éste hidden state es básicamente guardar en su ___memoria___ la información anterior de la secuencia. Más adelante veremos que éste tipo de estructuras no funcionan en secuencias muy largas, pero afortunadamente hay otras estructuras que se adaptan bastante bien como las __Long Short Term Memory Cells (LSTM)__ o las __Gated Recurrent Units (GRU)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061461d1",
   "metadata": {},
   "source": [
    "## Recordando Un Poco De Propiedades Matriciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb128aed",
   "metadata": {},
   "source": [
    "Comenzamos examinando algunos términos del esquema:\n",
    "* $x_{t}$ : Input _t_ de la secuencia\n",
    "* $W_{ax}$ : Matriz que multiplica al Input _t_ de la secuencia\n",
    "* $W_{aa}$ : Matriz que multiplica al Hidden State anterior _t-1_\n",
    "* $b_{a}$ : Bias term del estado de activación \n",
    "* $g_{1}$ : Activación 1\n",
    "* $W_{ya}$ : Matriz que multiplica al Hidden State _t_ de la secuencia para generar el Output _t_\n",
    "* $b_{y}$ : Bias term del estado de activación _t_\n",
    "* $g_{2}$ : Activación 2\n",
    "* $y_{t}$ : Output _t_ de la secuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54545b42",
   "metadata": {},
   "source": [
    "Para obtener el estado de activación, se realiza la siguiente operación: \n",
    "\n",
    "$a^{<t>}=g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$\n",
    "\n",
    "Sin embargo, matemáticamente hablando, lo anterior se puede también representar de la siguiente forma que es la cual habitualmente se muestra en muchas imágenes:\n",
    "\n",
    "$a^{<t>}=g(W_{a}[a^{<t-1>},x^{<t>}] + b_a)$    \n",
    "\n",
    "Ahora bien, el operador __[ , ]__ indica una concatenación vertical. Ésto quiere decir que los vectores, se insertan, uno al lado del otro. Consideremos los vectores $v^{<1>}$ y $v^{<2>}$. La operación $[v^{<1>},v^{<2>}]$ entrega lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94afaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89860195],\n",
       "       [0.59288451]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.random.rand(2,1)\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a68f90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84848954],\n",
       "       [0.87074064]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = np.random.rand(2,1)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3055c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89860195],\n",
       "       [0.59288451],\n",
       "       [0.84848954],\n",
       "       [0.87074064]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v12 = np.concatenate((v1, v2), axis=0)\n",
    "v12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2e20f",
   "metadata": {},
   "source": [
    "Ahora bien, las matrices $W_{aa}$ y $W_{ax}$ , se concatenan en la fórmula alternativa, pero de manera __horizontal__. De esta forma consideramos la matriz $W_{1}$ y $W_{2}$ , la cual se convierte en $W_{12}$ quedando de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fdd077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60648298, 0.64039855],\n",
       "       [0.30904415, 0.68210811]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = np.random.rand(2,2)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b912c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50733205, 0.87109552],\n",
       "       [0.51237973, 0.06587667]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = np.random.rand(2,2)\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee99bc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60648298, 0.64039855, 0.50733205, 0.87109552],\n",
       "       [0.30904415, 0.68210811, 0.51237973, 0.06587667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w12 = np.concatenate((w1 , w2) , axis = 1)\n",
    "w12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a01279",
   "metadata": {},
   "source": [
    "Ahora verificamos las fórmulas para demostrar la equivalencia:\n",
    "\n",
    "Fórmula 1 : $W_{1}v^{<1>} + W_{2}v^{<2>}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca3e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11363338],\n",
       "       [1.17422935]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula1 = np.matmul(w1 , v1) + np.matmul(w2 , v2)\n",
    "formula1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6e934",
   "metadata": {},
   "source": [
    "Fórmula 2 : $W_{12}[v^{<1>},v^{<2>}]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0cc23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11363338],\n",
       "       [1.17422935]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula2 = np.matmul(w12, v12)\n",
    "formula2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fea65b",
   "metadata": {},
   "source": [
    "__OBS__ : omití a propósito las funciones de activación solo para demostrar la equivalencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b178a",
   "metadata": {},
   "source": [
    "## Creando una Celda RNN con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89e064-6f16-45b4-9e68-e8586ef5183d",
   "metadata": {},
   "source": [
    "![Unfolded](unfolded.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddaffff",
   "metadata": {},
   "source": [
    "Crear una celda RNN como la de la figura en PyTorch es bastante sencillo. Solamente necesitamos 3 inputs:\n",
    "* __Input Length__ : El tamaño de entrada del input. Por ejemplo en el caso de las oraciones de texto sería el tamaño del embedding.\n",
    "* __Hidden Length__ : El tamaño del hidden vector\n",
    "* __Number of Layers__: Al igual que en un modelo denso habitual, aqui podemos tener un stack de capas de RNN y es en éste parámetro donde especificamos cuántas queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3df2e4-c8dc-48db-b969-a1fc8fa32f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnncel = nn.RNN(5,8,1 , batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ed072-29d0-47f6-a2a1-4ace8105884e",
   "metadata": {},
   "source": [
    "Como output de la celda obtenemos dos elementos. Por una parte obtenemos el __Output__ el cual entrega el hidden state de la última capa para cada item de la secuencia. En segundo lugar está el __hidden__ el cual contiene ___Todos___ los hidden states calculados para el último item de la secuencia. \n",
    "\n",
    "Revisando la documentación de __PyTorch__ podemos ver qué dimensiones requieren los inputs\n",
    "\n",
    "* __Input__: Dimensiones (tamaño del batch , tamaño de la secuencia , tamaño del input como por ejemplo longitud del embedding)\n",
    "* __Hidden State__: Dimensiones (numero de capas , tamaño del batch , tamaño del vector hidden)\n",
    "\n",
    "Consideremos como ejemplo el tensor $x$ correspondiente al embedding de una oración de dos palabras por eso tiene dimensiones (1 = un ejemplo , 2 = dos palabras , 5 = cada embedding de dimensión 5) . Recordar que __PyTorch__ funciona con __Tensors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d7e503-c047-4c8a-85b3-defe479946b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(np.ones((1,2,5)) , dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98d2ea-d4d1-4253-a7af-eb32ef5b780a",
   "metadata": {},
   "source": [
    "Ahora creamos el vector hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e619a2e-2962-4e01-90e1-895eefee6596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = torch.tensor(np.ones((1,1,8)) * 2 , dtype = torch.float32)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184ff95-73ec-412f-9d78-36361b148d8e",
   "metadata": {},
   "source": [
    "Ahora hacemos una propagación por la celda y obtenemos los outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "884a16f6-e13b-4723-9d98-5327b0f407f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      " tensor([[[-0.4620, -0.9650,  0.4580,  0.9926,  0.5424,  0.0114,  0.0017,\n",
      "          -0.4429],\n",
      "         [-0.1781, -0.3785,  0.5977,  0.5734,  0.8309,  0.6515,  0.7151,\n",
      "          -0.5111]]], grad_fn=<TransposeBackward1>)\n",
      "New Hidden :\n",
      " tensor([[[-0.1781, -0.3785,  0.5977,  0.5734,  0.8309,  0.6515,  0.7151,\n",
      "          -0.5111]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output , new_hidden = rnncel(x , hidden)\n",
    "print(f\"Output :\\n {output}\")\n",
    "print(f\"New Hidden :\\n {new_hidden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58a5f3-2c51-4f92-b65e-c71186d4dd46",
   "metadata": {},
   "source": [
    "Los outputs tienen las siguientes dimensiones:\n",
    "\n",
    "* __Output__: Dimensiones (tamaño del batch , tamaño de la secuencia , tamaño del vector hidden)\n",
    "* __Hidden State__: Dimensiones (numero de capas , tamaño del batch , tamaño del vector hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee36ad5-ad46-47fa-a0c1-d96501899fb9",
   "metadata": {},
   "source": [
    "## Abriendo las Matemáticas dentro de la Celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d4892-f8a4-43f0-a3e5-69d13420b16f",
   "metadata": {},
   "source": [
    "La celda nos permite obtener las matrices usadas en el cálculo. Primero extraemos manualmente el primer vector del primer input de la secuencia denotado como $x_{1}$ y hidden state inicial como $h_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7884cf2-ff49-4265-bfef-5827890f2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[0][0]\n",
    "h0 = hidden[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd5236-4c99-4b11-9e82-69448f7b983d",
   "metadata": {},
   "source": [
    "Luego obtenemos las matrices respectivas a partir de la celda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c84095ed-3cc2-4bec-8f1a-d7f2b7b021e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Waa1 = rnncel.weight_hh_l0\n",
    "Wia1 = rnncel.weight_ih_l0\n",
    "baa1 = rnncel.bias_hh_l0\n",
    "bia1 = rnncel.bias_ih_l0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea9dce-f333-40dd-80e3-263e59336b73",
   "metadata": {},
   "source": [
    "Realizando las multiplicaciones respectivas obtenemos el __Hidden Input 1__ para el primer elemento de la secuencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd3928a-88db-4e60-b3a9-d7f52cbcfc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4620, -0.9650,  0.4580,  0.9926,  0.5424,  0.0114,  0.0017, -0.4429],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input1_item1 = torch.tanh(torch.matmul(Waa1,h0) + bia1 + torch.matmul(Wia1,x1) + baa1)\n",
    "hidden_input1_item1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96b4a8-a352-48c7-a7fa-71a6da349a1d",
   "metadata": {},
   "source": [
    "Volvemos a realizar la operación con el segundo item de la secuencia $x_{2}$. Notar que aqui el vector hidden utilizado es el que salió del primer item (hidden_input1_item1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f282b2d-e678-4e58-8929-b6b6fd2c739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054db26d-d5f2-4f8b-9bd4-2e9ce7a97295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1781, -0.3785,  0.5977,  0.5734,  0.8309,  0.6515,  0.7151, -0.5111],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input2_item1 = torch.tanh(torch.matmul(Waa1,hidden_input1_item1) + bia1 + torch.matmul(Wia1,x2) + baa1)\n",
    "hidden_input2_item1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660e6f0-6bfe-4348-9464-de356472933f",
   "metadata": {},
   "source": [
    "De ésta forma podemos ver que los outputs coinciden con los que entrega la RNN. Notar que el item __Output__ nos entrega todos los estados intermedios calculados para el último input de la secuencia, en éste caso el hidden_input_1_item1 y hidden_input_2_item1, mientras que __New Hidden__ retorna solo el último hidden vector de la secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af9888f8-50c7-4936-968a-e565b1644a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      " tensor([[[-0.4620, -0.9650,  0.4580,  0.9926,  0.5424,  0.0114,  0.0017,\n",
      "          -0.4429],\n",
      "         [-0.1781, -0.3785,  0.5977,  0.5734,  0.8309,  0.6515,  0.7151,\n",
      "          -0.5111]]], grad_fn=<TransposeBackward1>)\n",
      "New Hidden :\n",
      " tensor([[[-0.1781, -0.3785,  0.5977,  0.5734,  0.8309,  0.6515,  0.7151,\n",
      "          -0.5111]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output :\\n {output}\")\n",
    "print(f\"New Hidden :\\n {new_hidden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f503c-4fee-4445-a698-43cecd92238e",
   "metadata": {},
   "source": [
    "Es importante notar que hay que añadir una capa __Densa__ dependiendo de si nuestro modelo es one-to-one , many-to-many, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04074e7-c13e-4b02-ba86-147401917f0c",
   "metadata": {},
   "source": [
    "## Creando un Stack de RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff048f-40aa-49b5-8ae1-65524ce454dc",
   "metadata": {},
   "source": [
    "El ejemplo anterior explicaba de qué forma una sola celda realiza una propagación hacia adelante para una secuencia de longitud dos. Ahora tomaremos el mismo ejemplo anterior, pero colocaremos otra celda RNN sobre la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "092b86c6-3a21-4b5b-bcbd-0151bc4ae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnceldoble = nn.RNN(5,8,2 , batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3f608-15e7-44a9-8474-ed98c6398537",
   "metadata": {},
   "source": [
    "En éste caso debemos crear otro vector hidden inicial ya que ahora tenemos dos celdas en vez de una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fba1cd8-b74d-4573-98e2-cf71c4e80ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddendoble = torch.tensor(np.ones((2,1,8)) * 2 , dtype = torch.float32)\n",
    "hiddendoble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4c82424-8931-4825-b38c-9d3be2cb6c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Doble RNN :\n",
      " tensor([[[ 0.3904,  0.7600, -0.4166,  0.8753, -0.8069,  0.9519,  0.6347,\n",
      "          -0.7350],\n",
      "         [-0.1631,  0.2121,  0.2217, -0.3851, -0.5885,  0.7872, -0.1114,\n",
      "           0.0641]]], grad_fn=<TransposeBackward1>)\n",
      "New Hidden Doble RNN :\n",
      " tensor([[[ 0.1707, -0.5239, -0.6099, -0.2825,  0.7655, -0.1127, -0.2522,\n",
      "           0.3227]],\n",
      "\n",
      "        [[-0.1631,  0.2121,  0.2217, -0.3851, -0.5885,  0.7872, -0.1114,\n",
      "           0.0641]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output2 , new_hidden2 = rnnceldoble(x , hiddendoble)\n",
    "print(f\"Output Doble RNN :\\n {output2}\")\n",
    "print(f\"New Hidden Doble RNN :\\n {new_hidden2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75270b-ad5e-446a-b62b-b0e11f3a73bd",
   "metadata": {},
   "source": [
    "Obtenemos las matrices respectivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7066e3-e607-490f-84ac-e5056d9a4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Waa_layer1 = rnnceldoble.weight_hh_l0\n",
    "Wia_layer1 = rnnceldoble.weight_ih_l0\n",
    "baa_layer1 = rnnceldoble.bias_hh_l0\n",
    "bia_layer1 = rnnceldoble.bias_ih_l0\n",
    "Waa_layer2 = rnnceldoble.weight_hh_l1\n",
    "Wia_layer2 = rnnceldoble.weight_ih_l1\n",
    "baa_layer2 = rnnceldoble.bias_hh_l1\n",
    "bia_layer2 = rnnceldoble.bias_ih_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb28f0a-a619-4ea3-8fb3-8bd10d736289",
   "metadata": {},
   "source": [
    "Comenzamos el cálculo con el primer input de la secuencia $x_{1}$. Recordar que ahora el vector es doble (por las 2 RNN). Denotamos al vector hidden inicial como $h_{00}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "537f702e-3f12-45ad-a48a-b0cf8ce67a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[0][0]\n",
    "h00 = hiddendoble[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11970024-6393-443b-82b8-942c7d3d775e",
   "metadata": {},
   "source": [
    "Calculamos el primer y segundo hidden state del primer input de la secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b50bfcd-9744-43d4-b23d-375b724f917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9161, -0.9770, -0.1195,  0.9145,  0.9790, -0.8160,  0.9171, -0.2385],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input1_item1_doble = torch.tanh(torch.matmul(Waa_layer1,h00) + bia_layer1 + torch.matmul(Wia_layer1,x1) + baa_layer1)\n",
    "hidden_input1_item1_doble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "561b0aab-4d03-48a4-b884-908c47a734cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector inicial para la segunda RNN\n",
    "h01 = hiddendoble[1][0]\n",
    "h01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "746ea025-e017-40b8-83fe-3dedc35ab6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3904,  0.7600, -0.4166,  0.8753, -0.8069,  0.9519,  0.6347, -0.7350],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input2_item1_doble = torch.tanh(torch.matmul(Waa_layer2,h01) + bia_layer2 + torch.matmul(Wia_layer2,hidden_input1_item1_doble) + baa_layer2)\n",
    "hidden_input2_item1_doble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6f501-cb01-4466-9765-af05d546125f",
   "metadata": {},
   "source": [
    "A continuación continuamos con el segundo input de la secuencia, denotado como $x_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00dc62ac-c2c2-45f1-b2be-f3effb7be8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ddeeb-caf3-4abb-8f89-65900e00ca67",
   "metadata": {},
   "source": [
    "Notar que el hidden vector inicial para el input 2 es el hidden_vector_1 generado con el input 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e8a0857-7ba0-4b95-aec3-efb9f1c1148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1707, -0.5239, -0.6099, -0.2825,  0.7655, -0.1127, -0.2522,  0.3227],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input1_item2_doble = torch.tanh(torch.matmul(Waa_layer1,hidden_input1_item1_doble) + bia_layer1 + torch.matmul(Wia_layer1,x2) + baa_layer1)\n",
    "hidden_input1_item2_doble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ef9de-3023-4bfc-9763-49e12b8d6ca6",
   "metadata": {},
   "source": [
    "Finalmente calculamos el output 2 del segundo input, en el cual se usan tanto los hidden states del input 1 como los generados anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f2a5695-d32f-47b6-a19a-5dd89aff50e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1631,  0.2121,  0.2217, -0.3851, -0.5885,  0.7872, -0.1114,  0.0641],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_input2_item2_doble = torch.tanh(torch.matmul(Waa_layer2,hidden_input2_item1_doble) + bia_layer2 + torch.matmul(Wia_layer2,hidden_input1_item2_doble) + baa_layer2)\n",
    "hidden_input2_item2_doble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d84dc-ba5b-4187-920b-987bc8cdac43",
   "metadata": {},
   "source": [
    "De ésta forma podemos observar que tanto los outputs como los hidden states calculados manualmente coinciden con los entregados por la celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44aca37b-a360-4214-87bf-f4c5aaf466a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3904,  0.7600, -0.4166,  0.8753, -0.8069,  0.9519,  0.6347,\n",
       "          -0.7350],\n",
       "         [-0.1631,  0.2121,  0.2217, -0.3851, -0.5885,  0.7872, -0.1114,\n",
       "           0.0641]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "726a798b-0f15-4394-a6ad-f9ac3642bd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1707, -0.5239, -0.6099, -0.2825,  0.7655, -0.1127, -0.2522,\n",
       "           0.3227]],\n",
       "\n",
       "        [[-0.1631,  0.2121,  0.2217, -0.3851, -0.5885,  0.7872, -0.1114,\n",
       "           0.0641]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4b4fd-8616-4c09-b63c-7dd7c59264b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
